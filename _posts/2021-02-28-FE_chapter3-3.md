---
layout: post
title:  "[FE for ML] 2.Text Data:Flattening, Filtering, and Chunking (3) 파싱, 토큰화, 연어 구문탐색"
subtitle: "[FE for ML] 2.Text Data:Flattening, Filtering, and Chunking (3) 파싱, 토큰화, 연어 구문탐색"
categories: data
tags: fe
mathjax: true
comments: true
---
<h3>Table of Contents<span class="tocSkip"></span></h3>
<div class="toc"><ul class="toc-item"><li><span><a href="#1.-파싱과-토큰화" data-toc-modified-id="1.-파싱과-토큰화-1">1. 파싱과 토큰화</a></span></li><li><span><a href="#2.-구문-탐색을-위한-연어-추출" data-toc-modified-id="2.-구문-탐색을-위한-연어-추출-2">2. 구문 탐색을 위한 연어 추출</a></span><ul class="toc-item"><li><span><a href="#2-1.-빈도-기반-방법-(비추천)" data-toc-modified-id="2-1.-빈도-기반-방법-(비추천)-2.1">2-1. 빈도 기반 방법 (비추천)</a></span></li><li><span><a href="#2-2.-연어-추출을-위한-가설-검증" data-toc-modified-id="2-2.-연어-추출을-위한-가설-검증-2.2">2-2. 연어 추출을 위한 가설 검증</a></span></li><li><span><a href="#2-3.-청킹과-품사-태깅" data-toc-modified-id="2-3.-청킹과-품사-태깅-2.3">2-3. 청킹과 품사 태깅</a></span></li></ul></li></ul></div>

본 포스팅은 『Feature Engineering for Machine Learning - PRRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS』의 내용을 바탕으로 구성하였으며 저의 주관적인 생각과 견해가 함께 서술되어 있습니다.


```python
# 사전 작업
import warnings
import pandas as pd
import matplotlib.pyplot as plt
warnings.filterwarnings(action='ignore')
plt.rcParams['axes.unicode_minus'] = False 
plt.rc('font', family='Malgun Gothic') 
```

<br>

---

`문자열`을 `단어의시퀀스`로 변환시키는 방법(`파싱(parsing)`, `토큰화(tokenization)`)에 관해서 살펴보자.

<br>

### 1. 파싱과 토큰화

- 문자열에 text이외의 것이 포함되어 있는 경우가 있다.
    - 이런 case에서 필요한 것이 **`파싱`**이다.
    - **문서**의 종류에 따라 관심없는 부분을 처리할 방법을 결정해야만 한다.
        - ex1) (문서 : 웹페이지) => URL : 의미없기에 처리
        - ex2) (문서 : 이메일) => From, To, Subject : 의미를 구분해주기에 특별 처리
        
<br>

- 간단한 `파싱`후에는 일반 **문서** text 부분은 **`토큰화`**를 수행할 수 있다.
    - `토큰화` : 글자의 시퀀스인 `문자열`을 `토큰의 시퀀스`를 변환
    - `토큰` : 각 토큰은 하나의 단어로서 카운트된다.
    - 토큰화 유용 구분자 : 공백 문자, 구두점(마침표, 쉼표)

<br>

때로는 `전체 문서` 대신 `문장`을 분석해야하는 경우도 있다.

- **`n-grams`로 $n$개의 토큰을 생성할 경우** `문서`가 아닌 `문장`단위로 수행되어져야 한다.
    - 즉, 문장 단위로 수행할 때 마침표(.)는 중요한 지표가 된다.
    - 하지만,`word2vec`와 같이 복잡한 피쳐 생성 기법은 `문장`이나 `단락`에 적용하는 경우가 있다.
    - 따라서, `문서 ->(파싱)-> 문장 ->(토큰화)-> 단어`에서 상황에 따라 필요한 것을 사용하도록 하자.

<br>

---

### 2. 구문 탐색을 위한 연어 추출

우선 **`연어(collocation)`**란?
- 어떤 것을 지칭하는 관습적인 방식에 부합하는 두 개 이상의 단어로 구성된 표현
    - ex1) 연어인 경우 : <br> "strong tea : 진한 차" ≠ strong(큰 물리적인 힘) + tea(차)
    - ex2) 연어가 아닌 경우 : <br>"cute puppy : 귀여운 강아지" = cute(귀여운) + puppy(강아지)
- 우리말로 생각해보면, 따로 쓰면 의미를 잃어버리는 구문을 의미한다.
    - ex) "학창 시절", "연인 사이" 등
- 연어는 연속일 수도 있고 아닐 수도 있다. 따라서 반드시 연속적인 시퀀스를 가져야할 필요는 없다.
    - 따라서 모든 연어가 `n-grams`인 것은 아니다.
    - ex) Emma Knocked on the door => 연어 : "Knock Door" ∉ n-grams


그러므로 `연어`를 `카운팅`을 통해 연어인 구문의 의미를 정확히 그리고 적절하게 잡아낼 수 없다. 따라서 `연어`를 텍스트에서 서치하기 위해 다음의 방법들을 사용한다.

- 1) 텍스트에서 사용되어진 `연어`를 미리 정의 
    - 많은 노력이 필요하겠지만 효과는 발군일 것이다.
    - But, 많은 수작업 및 지속적 업데이트가 진행되어져야만 한다.
    - 따라서 현실적이지는 않다...
- 2) **통계적 방법론**
    - NLP등장 이후 구문을 찾기 위해 위와 같은 수작업인 방법보다는 통계적인 방법론이 더 많이 선택되어지고 있다.

#### 2-1. 빈도 기반 방법 (비추천)


해당 방법은 가장 단순한 방법이다. 단지 <u>가장 빈번하게 나타나는 </u>`n-grams`<u>를 살펴보는 것이다</u>. 하지만, 가장 빈번하게 나타나는 것은 이전에도 확인했듯이 가장 유용하지 않을 수 있다는 리스크가 크다. 예를 들자면, 문장에서 가장 많이 확인할 수 잇는 "주어+동사"구문이 이에 해당할 것이다. 특히 3인칭 소설의 경우 연어라고 볼 수 없는 "I am", "I was" 등이 2-grams로 나타냈을 때 가장 많이 카운팅 될 것이다.

<br>

#### 2-2. 연어 추출을 위한 가설 검증

해당 기법에서의 핵심은 **<u>두 단어가 우연히 만날 확률보다 더 자주 함께 나타나는 묻는 것</u>**이다. 이 질의에 답하기 위해서는 `가설 검증( hypothesis test)`를 진행해야만 한다.

<br>

**`가설 검증`**이란?
- 잡음이 많은 데이터를 "YES"혹은 "NO"로 정제하는 방법이다.
- **무작위 분포에서 추출한 표본**을 통해 데이터를 모델링하는 과정도 포함된다.
    - 따라서 검증을 위한 답은 항상 **확률**로 표현될 것이다.
    - 가설 검증 결과 예시 : <br>이 두데이터셋은 95%의 확률로 동일한 분포로 부터 나왔다고 할 수 있다.

<br>

책에서 설명되어지는 `연어 추출`에 대한 가설 검증은 `우도 비율 검정(likelihood ratio test)`를 기반으로 한다.

<br>

**`우도 비율 검정`**이란?

- 두 단어 쌍에 대해서 이 기법은 관측된 데이터셋에서 **두 가지 가설**을 테스트하는 과정
    - `귀무가설` 
        - 설정한 가설이 진실할 확률이 극히 적어 처음부터 버려질 것이라 예상하는 가설
        - $word1$이 $word2$와 **독립적**으로 나타날 것이다.
    - `대립가설`
        - 귀무가설이 기각되어졌을 때, 받아들여지는 대체가설
        - 즉, 귀무가설과 달리 실제 검증 대상이 아니며 단순히 귀무가설이 기각되면 채택되어지는 가설
        - $word1$이 보이는 것은 $word2$를 보게 될 가능성을 **변화 시킬 것**이다.

<br>

`연어`추출을 위한 `우도 비율 검정`의 경우 다음과 같은 질문을 제기할 것이다.

> 주어진 corpus에서 해당 단어의 **발생 빈도**는 아래의 두 모델 중 어느 쪽이 더 높을 까?
> - 귀무가설 : 두 단어가 독립적으로 발생하는 모델
> - 대립가설 : 두 단어의 발생 확률이 얽혀 있는 모델


각 가설은 다음의 수식으로 표현할 수 있다.
- 귀무가설 $H_{null}$  <br>
	$$P(w_2|w_1) = P(w_2|{w_1}^c)$$
- 대립가설 $H_{alternate}$  <br>
	$$P(w_2|w_1) ≠ P(w_2|{w_1}^c)$$

최종 통계값인 이 둘의 **우도 비율 검정 통계량에 대한 로그**는 다음과 같다.
- 여기서 우도 함수 $L(Data;H)$는 단어 쌍에 대한 **독립**혹은 **비독립** 모델에서 데이터셋 내에 단어가 나타날 빈도의 확률의 의미한다.

$$\logλ = \log\frac{L(Data;H_{null})}{L(Data;H_{alternate})}$$

<br>(아직은 확실히 이해하지 못했다 조금 더 통계에 관해서 숙련도를 쌓은 뒤 다시 알아봐야겟다 ㅠ.ㅠ 지금은 이런게 있다! 정도로만 알고 넘어가야겠다...)

<br>

#### 2-3. 청킹과 품사 태깅

**`청킹`** 이란?
- `청크` 단위로 묶어 이해하는 과정을 `청킹`이라 한다.
    - 청크 : 머리 속에서 1개의 덩어리로 취급되는 단어 개념
- 품사를 기반으로 토큰 시퀀스를 형성, 이런 면에서 `n-grams`보다 좀 더 정교하다
- 품사로 각 단어를 토큰화한 후에 품사 그룹 또는 청크를 찾기 위해 토큰들을 검사한다.
- 이와 같이 단어와 품사를 매핑하는 모델은 일반적으로 언어에 종속적이다.


<br>

---

### References

- Alice Zheng, Amanda Casari, 『Feature Engineering for Machine Learning - PRRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS』, O'Relly Media (2018)

