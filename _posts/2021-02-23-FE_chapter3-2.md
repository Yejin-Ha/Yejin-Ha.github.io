---
layout: post
title:  "[FE for ML] 2.Text Data:Flattening, Filtering, and Chunking (2) 불용어, 빈도 기반 필터링"
subtitle: "[FE for ML] 2.Text Data:Flattening, Filtering, and Chunking (2) 불용어, 빈도 기반 필터링"
categories: data
tags: fe
mathjax: true
comments: true
---
<h3>Table of Contents<span class="tocSkip"></span></h3>
<div class="toc"><ul class="toc-item"><li><span><a href="#1.-불용어" data-toc-modified-id="1.-불용어-1">1. 불용어</a></span></li><li><span><a href="#2.-빈도-기반-필터링" data-toc-modified-id="2.-빈도-기반-필터링-2">2. 빈도 기반 필터링</a></span><ul class="toc-item"><li><span><a href="#2-1.-빈출-단어" data-toc-modified-id="2-1.-빈출-단어-2.1">2-1. 빈출 단어</a></span></li><li><span><a href="#2-2-희귀-단어" data-toc-modified-id="2-2-희귀-단어-2.2">2-2 희귀 단어</a></span></li><li><span><a href="#2-3.-어간-추출" data-toc-modified-id="2-3.-어간-추출-2.3">2-3. 어간 추출</a></span></li></ul></li></ul></div>

본 포스팅은 『Feature Engineering for Machine Learning - PRRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS』의 내용을 바탕으로 구성하였으며 저의 주관적인 생각과 견해가 함께 서술되어 있습니다.


```python
# 사전 작업
import warnings
import pandas as pd
import matplotlib.pyplot as plt
warnings.filterwarnings(action='ignore')
plt.rcParams['axes.unicode_minus'] = False 
plt.rc('font', family='Malgun Gothic') 
```

<br>

---

단어를 사용하여 데이터를 분석할 때, 의미있는 단어와 의미없는 잡음 단어를 깨끗하게 분리하기 위한 필터링 수단에 관하여 알아보자.
<br>

### 1. 불용어

`불용어 리스트`를 이용해서 의미 없는 피처를 생성하는 잡음 단어들을 제거할 수 있다.

<br>

그렇다면 **`불용어(stopword)`**란 무엇일까?
- I, on, the, me, 조사, 접미사와 같이 문장에서 자주 등장하지만 <u>제외시켜도 문장의 의미를 바꾸지 않는 요소</u> 혹은 자주 등장하지만 <u>실제적으로는 문장을 분석하는데 있어서 큰 영향을 주지 않는 단어</u>들을 의미한다.
    - 위와 같은 대명사/관사/전치사는 `분류`작업에서는 가치가 크지않다. 하지만, 정해진 규칙이 없어 의미를 예측하기 힘든 비정형 데이터를 분석하기 위한 **감성분석(sentiment analysis)**에서는 가치있을 수 있다. (여기서는 감성분석에 관해서 다루지 않는다)
    - 파이썬의 `NLP`패키지인 `NLTK`는 많은 언어학자들이 정의한 `불용어 리스트`를 가지고 있다. (안타깝지만 한글은 절레절레...)
    - ex) a, about, above, am, an, been, didn't, couldn't, i'd, i'll, itself, let's, myself, our, they ...
   
<br>

또 다른 잡음 필터링 방법으로는 **`상용어(common word)`**를 사용하는 **통계적인 방법**도 존재한다.

<br>

(*상용어 : 일상에서 자주 쓰이는 언어)

<br>

---

### 2. 빈도 기반 필터링

#### 2-1. 빈출 단어

얼마나 단어가 카운팅되는지 확인하는 `빈도 통계`를 이용하게되면 `상용어`혹은 `불용어`인 단어를 필터링하기에 유용하다. 하지만, **<u>특정 단어가 너무 많이 카운팅될 경우 의미를 파악하기 어려울 수 있다</u>**. 예를 들어, 케나다의 의사 회의록에서는 "House of Commons(하원)"이라는 문구가 많기 때문에 "House"라는 단어의 카운팅이 높다. 하지만, "House of Commons"에서의 "House"는 우리가 생각하는 일반적인 "집"을 의미하지 않는다.

<br>

이처럼 "House"와 같은 일반적으로는 의미를 가지고 있지만, 특정 corpus(ex."House of Commons")에서는 그렇지 않은 단어같이 **corpus에 종속적인 단어**들은 `빈도 통계`를 이용해 구분해내기 힘들다.

<br>

이런 구분해내기 힘든 단어들을 어떻게 처리할지 생각하는 것도 꽤나 고난에 해당한다. 하지만, 단점만 있는 것은 아니다. 1)가장 빈번하게 나오는 단어를 살펴봄으로써 파싱 문제를 서치할 수 있고, 2)빈출이 가장 많이 되는 단어에 초점을 맞추어 문서를 효율적으로 확인할 수 있다.

<br>

#### 2-2 희귀 단어

목적에 따라서 `희귀 단어`를 필터링 하는 경우도 있다.

<br>

**`희귀 단어`**란?
- 정말 잘 알려지지 않은 단어
- 철자가 틀린 단어 등...
- 통계적 모델에서는 소수의 문서에서 나오는 unique한 단어에 해당하기에 유용하기보다는 잡음에 가깝다.

<br>

이러한 `희귀 단어`들은 모델의 연산량과 저장 비용을 증가 시킬 뿐더러, 분류와 같은 작업에서도 문제를 일으킬 수 있다.

> 이전에 계속 사용해왔던 Yelp 데이터셋의 경우<br>
> 160만개의 리뷰 중에서 357,481개의 고유 단어가 내제되어있으며, 378,481개라는 고유 단어 중 189,915개는 하나의 리뷰에만 나타나며, 41,162개의 단어는 오로지 한 두개의 리뷰에서만 나타난다고 한다.<br>
> <br>
> 즉 고유 단어 중 60% 이상이 희귀 단어에 속한다고 볼 수 있다. 즉 많이 나오는 단어의 갯수는 적고, 드물게 나타나는 단어의 갯수가 많다는 것이다. <br>
> <br>
> 이런 단어 데이터셋을 그대로 분류 모델에 적용할 시 과적합은 물론이고 계산 오버헤드도 유발할 것이다.


<br>

이처럼 데이터 사용자를 불편하게 만드는 `희귀 단어`는 다행히 **단어 카운트 통계**를 기반으로 쉽게 식별하고 정리할 수 있다. 또한 `희귀 단어` 카운트는 나중에 사용할 수 있도록 `휴지통(garbage bin)`에 모아 놓을 수 있다. 

<br>

더불어, 텍스트 문서가 매우 짧은 경우에는 해당 문서에는 유용한 정보가 없을 가능성이 높기 때문에 모델을 학습시킬 때 사용하지 않는 것이 좋다. 하지만, 트위터는 본질적으로 글이 짧기에 다른 피처 생성 기법 및 모델링 기법을 필요로 한다.

<br>

#### 2-3. 어간 추출


일련의 문자열을 의미있는 토큰으로 분해하고 BoW를 만드는 파싱 과정에서는 한 가지의 문제점이 있다. 그것은 "flower"과 "flowers", "swimmer"와 "swim"같은 **<u>동일한 단어의 여러가지 변형이 별도로 카운트 된다는 점</u>**이다.

<br>

"swimmer", "swimming", "swim" 모두 품사적으로는 모두 다르지만 단어의 의미는 서로가 매우 비슷하다. 따라서, **<u>단어의 여러 변형을 모두 하나의 동일한 어간(stem)으로 매핑</u>**되는 것이 좋을 수 있다.

(*어간 : 단어를 활용할 때 변하지 않는 부분)

<br>

위와 같이 동일한 `어간`으로 매핑하는 작업을 **`어간 추출(stemming)`**이라 부르며, 모든 단어의 기본이 되는 언어학적인 어근 형태로 잘라내는 NLP작업이다. 일부는 언어학적 규칙에 기반하며, 또 일부는 통계학에 기반을 둔다.

(*어근 : 단어의 중심이 되는 형태소)

<br>

대부분 어간 추출 도구는 영어에 집중되어 있다. 그중 많이 사용되는 **`포터 어간 추출기(Porter Stemmer)`**는 영어에 대한 무료 어간 추출 도구다. 다음은 `NTLK`파이썬 패키지를 통해 포터 어간 추출기를 실행하는 예제이다.


```python
# 만약 nltk모듈이 없다고 에러가 뜨면 다음을 수행하자
# import sys
# !{sys.executable} -m pip install nltk

import nltk
stemmer = nltk.stem.porter.PorterStemmer()

print('flowers :', stemmer.stem('flowers'))
print('zero :', stemmer.stem('zeros'))
print('stemmer :', stemmer.stem('stemmer'))
print('sixties :', stemmer.stem('sixties'))
print('sixty :', stemmer.stem('sixty'))
print('goes :', stemmer.stem('goes'))
print('go :', stemmer.stem('go'))
print('news :', stemmer.stem('news'))
print('new :', stemmer.stem('new'))
```

    flowers : flower
    zero : zero
    stemmer : stemmer
    sixties : sixti
    sixty : sixti
    goes : goe
    go : go
    news : news
    new : new
    

위의 예제에서 확인할 수 있듯이 많은 경우를 처리할 수 있지만 "goes"와 같이 완벽하지 않은 케이스들이 존재한다ㅠㅠ. 또한, 위의 분류기에서는 제대로 분류되었지만, "news"와 "new"는 서로 다른 의미를 가지고 있지만 두 단어의 동일 어간은 "new"이다. 이와 비슷한 예는 많이 존재한다. 그러므로 **<u>어간 추출이 항상 사용되어져야만 한다는 것은 아니라는 점을 기억해 두는 것이 좋다.</u>**


<br>

---

### References

- Alice Zheng, Amanda Casari, 『Feature Engineering for Machine Learning - PRRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS』, O'Relly Media (2018)

