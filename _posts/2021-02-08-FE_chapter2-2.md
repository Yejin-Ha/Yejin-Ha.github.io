---
layout: post
mathjax: true
title:  "[FE for ML] 1.Fancy Tricks with Simple Numbers (2) 카운트 처리/분산 안정화 변환"
subtitle: "[FE for ML] 1.Fancy Tricks with Simple Numbers (2) 카운트 처리/분산 안정화 변환"
categories: data
tags: fe
comments: true
---
#### Table of Contents
<div class="toc"><ul class="toc-item"><li><span><a href="#1.-카운트-처리" data-toc-modified-id="1.-카운트-처리-1">1. 카운트 처리</a></span><ul class="toc-item"><li><span><a href="#1-1.-바이너리-변환" data-toc-modified-id="1-1.-바이너리-변환-1.1">1-1. 바이너리 변환</a></span></li><li><span><a href="#1-2.-양자화-또는-비닝" data-toc-modified-id="1-2.-양자화-또는-비닝-1.2">1-2. 양자화 또는 비닝</a></span></li></ul></li><li><span><a href="#2.-로그-변환" data-toc-modified-id="2.-로그-변환-2">2. 로그 변환</a></span></li><li><span><a href="#3.-로그-변환의-역할" data-toc-modified-id="3.-로그-변환의-역할-3">3. 로그 변환의 역할</a></span></li><li><span><a href="#4.-거듭제곱-변환-:-로그-변환의-일반화" data-toc-modified-id="4.-거듭제곱-변환-:-로그-변환의-일반화-4">4. 거듭제곱 변환 : 로그 변환의 일반화</a></span></li></ul></div>

본 포스팅은 『Feature Engineering for Machine Learning - PRRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS』의 내용을 바탕으로 구성하였으며 저의 주관적인 생각과 견해가 함께 서술되어 있습니다.


```python
# 사전 작업
import warnings
import matplotlib.pyplot as plt
warnings.filterwarnings(action='ignore')
plt.rcParams['axes.unicode_minus'] = False 
plt.rc('font', family='Malgun Gothic') 
```

<br>

---

### 1. 카운트 처리

아무런 제약 없이 빠르게 누적되는 `카운트(count)`에는 다음과 같은 변환 수단들이 생각되어 질 수 있다.
- 데이터를 원시 숫자로 유지하는 수단
- 존재 여부만을 나타내는 binary로 변환하는 수단
- 몇 개의 구간으로 분할하는 수단

이를 설명하기 위해 몇 가지 예시를 살펴보자.

<br>

#### 1-1. 바이너리 변환

다음 데이터셋은 사용자 ID, 곡 ID, 재생 카운트라는 세 가지 값으로 이뤄진 데이터(4,800만건 이상)이다. 해당 데이터에서 사용자 선호도에 대한 표현으로 `바이너리화(binarization)`해서 1보다 큰 모든 재생 카운트를 1로 고정시켜보자.


```python
import pandas as pd
listen_count = pd.read_csv('./example_data/train_triplets.txt.zip', header = None, delimiter = '\t')
listen_count.columns = ['user_id', 'sound_track_num', 'play_count']
# 근데 초반 예시의 play_count가 대부분 1인점이 좀 아쉽다. (설명을 위한 부분에서)
listen_count.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user_id</th>
      <th>sound_track_num</th>
      <th>play_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOAKIMP12A8C130995</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOAPDEY12A81C210A9</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOBBMDR12A8C13253B</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOBFNSP12AF72A0E22</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOBFOVM12A58A7D494</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



해당 테이블에서는 재생 카운트가 0이 아닌 것들만 포함되었기 때문에 play_count열을 모두 1로 만들면된다.


```python
listen_count['play_count'] = 1
listen_count.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user_id</th>
      <th>sound_track_num</th>
      <th>play_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOAKIMP12A8C130995</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOAPDEY12A81C210A9</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOBBMDR12A8C13253B</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOBFNSP12AF72A0E22</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>
      <td>SOBFOVM12A58A7D494</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



바이너리 목표 변수는 사용자 선호도에 대한 단순하면서도 견고한 척도가 될 수 있다.

<br>

#### 1-2. 양자화 또는 비닝


```python
import pandas as pd
import json

biz_file = open('./example_data/archive/yelp_academic_dataset_business.json', encoding = 'utf-8-sig')
blz_df = pd.DataFrame([json.loads(x) for x in biz_file.readlines()])
biz_file.close()

blz_df.columns
```




    Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',
           'latitude', 'longitude', 'stars', 'review_count', 'is_open',
           'attributes', 'categories', 'hours'],
          dtype='object')



현재 가져온 Yelp 데이터셋은 북아메리카와 유럽의 열 개 도시에 있는 다양한 비즈니스에 대한 사용자의 리뷰를 포함하고 있다. 이 데이터를 이용하여 <u>사용자가 한 비즈니스에 줄 것으로 예상되는 등급</u>을 예측한다는 프로젝트를 수행한다 가정했을 때, `원시 리뷰 카운트를 그대로 사용할 것`인지 아니면 `추가적인 처리가 필요할 것`인지에 관해서 고민해보자.

<br>


```python
import matplotlib.pyplot as plt

plt.hist(blz_df['review_count'], bins = 100)
plt.xlabel('리뷰 카운트')
plt.ylabel('발생 건수')
plt.yscale('log')
plt.show()
```


![FE_chapter2-2_14_0](https://user-images.githubusercontent.com/53929665/107151674-1eac5f00-69a7-11eb-940e-32bf7ef36080.png)


위는 비즈니스 리뷰 카운트에 대한 히스토그램이다. 위와 같은 여러 자릿수에 걸쳐 폭넓게 펼쳐져 있는 원시 카운트 값은 많은 모델에서 많은 문제를 야기 할 수 있다. 예를 들어, <u>데이터 벡터에서 큰 값을 갖는 하나의 요소는 다른 모든 요소들의 유사도 보다 더 큰 영향력을 갖기 때문에 전체적으로 유사도 측정값을 왜곡 시킬 수 있다</u>.

<br>

이에 대한 해결책으로 카운트를 <u>연속적인 수를 일정한 폭을 갖는 불연속적인 이산화된 값으로 매핑</u>하는 `양자화(quantization)`시켜 bin으로 그룹화하는 것이다.

<br>

데이터를 `양자화`하기 위해서는 각 bin의 너비를 결정해야 한다. 이를 위한 기법들은 다음과 같다.
- **(고정 폭 방식 中) Fixed-width binning**
    - 각 빈은 <u>특정 범위의 수</u>를 포함한다. 
    - `범위`는 <u>사용자가 정의</u>하거나 <u>선형/지수 스케일</u>로 설정할 수 있다.
    - 사용자 정의 예시) bin1 : 0-9세, bin2 : 10-19세, ... (10년 단위로 그룹)<br>선형/지수 설정 예시) 0-9, 10-99, 100-999, 1000-9999, ... (10의 제곱으로 그룹)
    
- **(적응형 방식 中) Quantile binning**
    - 카운트 사이에 큰 갭이 있을 경우 Fixed-width binning은 데이터가 없는 bin을 많이 생성한다. 이 문제는 데이터의 분포를 기반으로 bin을 유동적(adaptively)으로 Quantile binning을 이용하여 배치해 해결할 수 있다.

<br>

예제에서 `Fixed-width binning`와 `Quantile binning`을 확인해보자


```python
# Fixed_width binning 카운트 양자화

import numpy as np

small_counts = np.random.randint(0, 100, 20)
print('>>> (1) 0-99 사이에서 20개의 무작위 정수 생성 :', small_counts, '\n')


# 10으로 나누기 해서 동일한 구간을 갖는 빈 0-9 매핑
print('>>> (1) 10으로 나눠 bin 0-9로 매핑 :',np.floor_divide(small_counts, 10), '\n')

large_counts = [295, 1234, 1523, 1423, 16, 1523, 612398, 123498, 4231123, 523, 2431, 1523]
print('>>> (2) 여러 자릿수에 걸쳐 있는 카운트 배열 생성 :', large_counts, '\n')

# 로그 함수를 통해 지수 폭 빈에 매핑
print('>>> (2) 로그함수를 이용해 bin에 매핑 :', np.floor(np.log(large_counts)))
```

    >>> (1) 0-99 사이에서 20개의 무작위 정수 생성 : [51  8 56 23 52 34 60 90 83 82 52 24 13 36  9  8 43 60 42 74] 
    
    >>> (1) 10으로 나눠 bin 0-9로 매핑 : [5 0 5 2 5 3 6 9 8 8 5 2 1 3 0 0 4 6 4 7] 
    
    >>> (2) 여러 자릿수에 걸쳐 있는 카운트 배열 생성 : [295, 1234, 1523, 1423, 16, 1523, 612398, 123498, 4231123, 523, 2431, 1523] 
    
    >>> (2) 로그함수를 이용해 bin에 매핑 : [ 5.  7.  7.  7.  2.  7. 13. 11. 15.  6.  7.  7.]
    

<br>


```python
# Quantile binning 카운트 양자화

deciles = blz_df['review_count'].quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])
deciles_df = pd.DataFrame({'십분위 수': [.1, .2, .3, .4, .5, .6, .7, .8, .9], 'review_count' : list(deciles)})
deciles_df = deciles_df.set_index('십분위 수')
deciles_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review_count</th>
    </tr>
    <tr>
      <th>십분위 수</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.1</th>
      <td>3.0</td>
    </tr>
    <tr>
      <th>0.2</th>
      <td>4.0</td>
    </tr>
    <tr>
      <th>0.3</th>
      <td>5.0</td>
    </tr>
    <tr>
      <th>0.4</th>
      <td>7.0</td>
    </tr>
    <tr>
      <th>0.5</th>
      <td>9.0</td>
    </tr>
    <tr>
      <th>0.6</th>
      <td>14.0</td>
    </tr>
    <tr>
      <th>0.7</th>
      <td>21.0</td>
    </tr>
    <tr>
      <th>0.8</th>
      <td>36.0</td>
    </tr>
    <tr>
      <th>0.9</th>
      <td>77.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
import seaborn as sns

sns.set_style('whitegrid')
fig, ax = plt.subplots()
blz_df['review_count'].hist(ax=ax, bins = 100)
for pos in deciles:
    handle = plt.axvline(pos, color = 'r')
ax.legend([handle], ['delices'], fontsize = 14)
ax.set_yscale('log')
ax.set_xscale('log')
ax.tick_params(labelsize = 14)
ax.set_xlabel('Review Count', fontsize = 14)
ax.set_ylabel('Occurrence', fontsize = 14)
```




    Text(0, 0.5, 'Occurrence')




![FE_chapter2-2_20_1](https://user-images.githubusercontent.com/53929665/107151678-1f44f580-69a7-11eb-8ad9-fc36a997034b.png)


<br>

분위수를 계산하고 데이터를 각 빈에 매핑하기 위해 아래와 같은 라이브러리를
- `padnas.DataFrame.quantile`
- `pandas.Series.quantile`
- `pandas.qcut` (**데이터를 원하는 수 만큼의 분위를 매핑)

사용할 수 있다.


```python
import pandas as pd

print('사용될 리스트 :', large_counts, '\n')

# 카운트를 사분위수와 매핑
print('>>> 사분위 매핑 :', pd.qcut(large_counts, 4, labels=False), '\n')

# 분위수 계산
large_counts_series = pd.Series(large_counts)
print('>>> 리스트를 토대로 각 분위수 계산 :')
large_counts_series.quantile([0.25, 0.5, 0.75])
```

    사용될 리스트 : [295, 1234, 1523, 1423, 16, 1523, 612398, 123498, 4231123, 523, 2431, 1523] 
    
    >>> 사분위 매핑 : [0 1 1 1 0 1 3 3 3 0 2 1] 
    
    >>> 리스트를 토대로 각 분위수 계산 :
    




    0.25     1056.25
    0.50     1523.00
    0.75    32697.75
    dtype: float64



<br>

---

### 2. 로그 변환

지수의 역함수인 로그를 사용하는 `로그 변환`은 `지수 폭 비닝`과 매우 밀접하게 관련되어있다. base=10인 log함수의 경우 다음과 같이 범위들을 매핑한다.

- [1, 10] 범위 => [0, 1] 범위
- [10, 100] 범위 => [1, 2] 범위
- [100, 1000] 범위 => [2, 3] 범위

이처럼 로그 함수는 <u>큰 수의 범위를 압축</u>하고 <u>작은 수의 범위를 큰 수의 범위에 비해 상대적으로 확장</u>한다.

<br>

다음의 Yelp 비즈니스 리뷰 카운트에 로그변환을 이용하기 전과 후의 히스토그램을 살펴보자.


```python
fig, (ax1, ax2) = plt.subplots(2,1, figsize = (8, 10))
blz_df['review_count'].hist(ax=ax1, bins=100)
ax1.tick_params(labelsize=14)
ax1.set_title('before log', fontsize = 14)
ax1.set_xlabel('review_count', fontsize=14)
ax1.set_ylabel('Occurrence', fontsize=14)


blz_df['log_review_count'] = np.log10(blz_df['review_count'] +1)
blz_df['log_review_count'].hist(ax=ax2, bins=100)
ax2.tick_params(labelsize=14)
ax2.set_title('after log', fontsize = 14)
ax2.set_xlabel('log(review_count)', fontsize=14)
ax2.set_ylabel('Occurrence', fontsize=14)

plt.tight_layout()
```


![FE_chapter2-2_25_0](https://user-images.githubusercontent.com/53929665/107151679-1fdd8c00-69a7-11eb-9224-09d46970c8cb.png)


원본 리뷰 카운트에는 낮은 카운트 영역에 매우 집중되어 있지만, 로그 변환을 한 히스토그램에서는 <u>낮은 쪽에 집중되는 현상이 줄어들었으며, x축 방향으로 데이터들의 분포가 펼쳐지는 것</u>을 확인할 수 있다.

<br>

또 다른 [데이터셋](https://archive.ics.uci.edu/ml/datasets/online+news+popularity)을 로드하여 확인해보자. (이 데이터셋은 Mashable사에서 약 2년간 발행한 39797개의 뉴스기가사가 포함되어 있으며 무려 60개의 피쳐를 가지고 있다.)


```python
df = pd.read_csv('./example_data/OnlineNewsPopularity.csv')

fig, (ax1, ax2) = plt.subplots(2,1, figsize = (8, 10))

df[' n_tokens_content'].hist(ax=ax1, bins = 100)
ax1.tick_params(labelsize=14)
ax1.set_xlabel('Number of Words in Article', fontsize = 14)
ax1.set_ylabel('Number of Articles', fontsize = 14)
ax1.set_title('before log', fontsize = 14)

df['log_n_tokens_content'] = np.log10(df[' n_tokens_content']+1)
df['log_n_tokens_content'].hist(ax=ax2, bins = 100)
ax2.tick_params(labelsize=14)
ax2.set_xlabel('Log of Number of Words', fontsize = 14)
ax2.set_ylabel('Number of Articles', fontsize = 14)
ax2.set_title('after log', fontsize = 14)

plt.tight_layout()
```


![FE_chapter2-2_28_0](https://user-images.githubusercontent.com/53929665/107151680-1fdd8c00-69a7-11eb-93ef-55a7453a357d.png)


위의 그림은 해당 데이터에 대한 로그 변환 이전과 이후의 히스토그램이다. 길이가 0일 때의 기사를 제외하면 <u>로그 변환 이후의 분포가 더 가우시안 분포와 유사</u>하다. 

<br>

---

### 3. 로그 변환의 역할

로그 변환은 주로 아래와 같은 상황에서 자주 쓰인다.
- 차이가 급격하게 벌어져 과거/비교 데이터와의 차이를 시각적으로 파악하기 위해 사용
- 절대치가 아닌 변화율을 확인할 때 사용 (예를 들자면 주식이 이에 부합한다.)

<br>

그렇다면, `지도학습`에서는 로그 변환이 어떻게 수행되는지도 살펴보자. 이를 위해 앞의 데이터를 이용하여 로그 변환 이전과 이후의 피처를 사용해서 선형 회귀 모델에 대한 교차 검증을 아래와 같이 수행한다.


```python
# 로그 변환된 Yelp 리뷰 카운트를 사용해 비즈니스의 평균 등급 예측

import pandas as pd
import numpy as np
import json
from sklearn import linear_model
from sklearn.model_selection import cross_val_score

# 변환 전
m_orig = linear_model.LinearRegression()
scores_orig = cross_val_score(m_orig, blz_df[['review_count']], blz_df['stars'], cv = 10)

# 변환 후
m_log = linear_model.LinearRegression()
scores_log = cross_val_score(m_log, blz_df[['log_review_count']], blz_df['stars'], cv = 10)

print('>>> for Yelp Data')
print('R-squared score without log transform : %0.5f (+/- %0.5f)'%(scores_orig.mean(), scores_orig.std()*2))
print('R-squared score with log transform : %0.5f (+/- %0.5f)'%(scores_log.mean(), scores_log.std()*2))

print('\n\n')

# 변환 전
m_orig = linear_model.LinearRegression()
scores_orig = cross_val_score(m_orig, df[[' n_tokens_content']], df[' shares'], cv = 10)

# 변환 후
m_log = linear_model.LinearRegression()
scores_log = cross_val_score(m_log, df[['log_n_tokens_content']], df[' shares'], cv = 10)

print('>>> for Online News Popularity Data')
print('R-squared score without log transform : %0.5f (+/- %0.5f)'%(scores_orig.mean(), scores_orig.std()*2))
print('R-squared score with log transform : %0.5f (+/- %0.5f)'%(scores_log.mean(), scores_log.std()*2))
```

    >>> for Yelp Data
    R-squared score without log transform : 0.00239 (+/- 0.00160)
    R-squared score with log transform : 0.00580 (+/- 0.00251)
    
    
    
    >>> for Online News Popularity Data
    R-squared score without log transform : -0.00242 (+/- 0.00509)
    R-squared score with log transform : -0.00114 (+/- 0.00418)
    

먼저 Yelp데이터와 News데이터에서 모두 로그를 적용하기 전과 후 똑같이 성능이 좋지는 못하지만, 두 데이터셋 모두 log를 적용한 후에 점수가 향상한 것을 확인할 수 있다.

<br>

이처럼 `로그 변환`이 효과적으로 작용한 이유는 아래의 예제를 통해서 확인할 수 있다.


```python
fig2, (ax1, ax2) = plt.subplots(2, 1, figsize = (8, 10))

ax1.scatter(df[' n_tokens_content'], df[' shares'])
ax1.tick_params(labelsize = 14)
ax1.set_title('before log', fontsize = 14)
ax1.set_xlabel('Number of Words in Article', fontsize = 14)
ax1.set_ylabel('Number of Shares', fontsize = 14)
ax1.axhline(y = 200000, linestyle = '--', color = 'red')
ax1.axvline(x = 688, linestyle = '--', color = 'red')

ax2.scatter(df['log_n_tokens_content'], df[' shares'])
ax2.tick_params(labelsize = 14)
ax2.set_title('after log', fontsize = 14)
ax2.set_xlabel('Log of Number of Words in Article', fontsize = 14)
ax2.set_ylabel('Number of Shares', fontsize = 14)
ax2.axhline(y = 200000, linestyle = '--', color = 'red')
ax2.axvline(x = np.log10(688), linestyle = '--', color = 'red')

plt.tight_layout()
```


![FE_chapter2-2_35_0](https://user-images.githubusercontent.com/53929665/107151681-20762280-69a7-11eb-8f05-f887207cbe45.png)


위에서 볼 수 있듯이, `로그 변환`은 공유 횟수 200000 이상인 article들이 오른쪽으로 이동한 것을 볼 수 있다. 이는 <u>입력 피처 공간의 낮은 쪽에 선형 모델이 숨쉴 수 있는 공간을 제공</u>한다. 하지만, `로그 변환을 하지 않은` 히스토그램과 같은 데이터에 선형 모델을 적용하게 될 경우 입력 피처의 작은 변화에도 다른 타깃값에 피팅되어 버릴 수 있다.

<br>

위의 결과를 Yelp 리뷰 데이터 셋에 적용한 산점도와 비교해보자.


```python
fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (8, 10))

ax1.scatter(blz_df['review_count'], blz_df['stars'])
ax1.tick_params(labelsize = 14)
ax1.set_title('before log', fontsize = 14)
ax1.set_xlabel('Review Count', fontsize = 14)
ax1.set_ylabel('Average Star Rating', fontsize = 14)

ax2.scatter(blz_df['log_review_count'], blz_df['stars'])
ax2.tick_params(labelsize = 14)
ax2.set_title('after log', fontsize = 14)
ax2.set_xlabel('Review Count', fontsize = 14)
ax2.set_ylabel('Average Star Rating', fontsize = 14)

plt.tight_layout()
```


![FE_chapter2-2_39_0](https://user-images.githubusercontent.com/53929665/107151682-210eb900-69a7-11eb-86b7-4213cdf5f986.png)


<u>많은 리뷰 카운트가 높은 평균 별점과 상관관계</u>를 가지기는 하지만 `선형`이라고는 볼 수 없다. <u>왜냐하면 두 입력 중 하나를 기준으로 평균 별점을 예측하는 선을 분명하게 나타낼 방법이 없기 때문이다</u>. 따라서, 로그를 취하던 안취하던 Yelp데이터에서 Review Count는 Average Star Rating에 대해 나쁜 선형 예측자 임을 보여준다.

<br>

---

### 4. 거듭제곱 변환 : 로그 변환의 일반화

`거듭제곱 변환`으로 알려진 기법 중 하나인 `로그 변환`은 통계 용어로는 `분산 안정화 변환(variance-stabilizing transformation)`이라 한다.

<br>

그렇다면, `분산 안정화 변환`이 왜 좋은 것일까? 그에 대한 답을 알기 위해서는 간단하게라도 `푸아송 분포`을 알아두어야 한다.

<br>

간단하게 `푸아송 분포`란?
- 단위 시간 내에 어떤 사건이 발생할 횟수를 확률변수로 가지는 확률 분포<br>=> 어떤 사건이 일어날 횟수에 대한 기댓값을 λ라 할 때 , 그 사건이 x 번 발생할 확률 

- 푸아송 분포의 확률변수 : 기댓값 = 분산 = λ<br> λ⇧ ∝ (분산⇧ & 꼬리굵기⇧)

- 주의) 이산형 데이터라 해도 무조건 포아송 분포는 아니다!

<br>

우선 `분산 안정화 변환`의 목적은 어떤 데이타셋안에 있는 값 `x`를 <u>그 데이터셋의 평균과 관련없는 가변 변수</u> `y`(= f(x))로 만드는 간단한 function `f`를 찾기 위함이다.

<br>

여기서 만약 데이터셋이 서로 다른 `포아송 분포`를 가진다고 생각해보자(즉 서로다른 기댓값 λ를 가집니다). 그러면 `포아송 분포`의 경우 위에서 언급했듯이 분산이 평균과 동일하기 때문에 평균에 따라서 분산이 달라질 것이다. 따라서 데이터셋은 `정규분포`를 따르지 않을 것이다. 이럴 경우에 데이터셋을 `회귀분석`하고자 한다면, `회귀분석`은 정규성 가정 하에서 성립되기 때문에 비정규성 문제가 발생할 수 있다.

<br>

그러나, 여기에 `분산 안정화 변환`이 적용된다면 
- <u>분산이 더 이상 평균에 의존하지 않게 된다는 이점</u>이 생길 뿐만 아니라 
- <u>변환되어진 변수의 분포가 정규분포에 가깝도록하는 이점</u>도 가진다.


$$ y = \sqrt{x} , y =\log{x} $$

<br>

제곱근 변환과 로그 변환의 가장 간단한 일반화로는 `Box-Cox`이 있다.

![img](https://user-images.githubusercontent.com/53929665/107151685-223fe600-69a7-11eb-9b74-ac81886825c9.png)

- 데이터가 양수일 때만 동작 (양수가 아닌 데이터는 고정된 상수를 더해줌으로써 가능)
- `SciPy`의 `stats`패키지에서 Box-Cox 변환에 대한 구현 제공


```python
from scipy import stats

# 입력 파라미터 lmbda를 0 으로 설정하면 로그 변환을 한다. (상수 오프셋x)
rc_log = stats.boxcox(blz_df['review_count'], lmbda = 0)

# SciPy에서 boxcox 변환은 출력을 정규분포에 가장 가깝게 만드는
# 람다파라미터(아래 코드에서는 bc_params에 저장됨)를 색인한다.
# rc_bc의 경우 변환된 값이 저장된다.
rc_bc, bc_params = stats.boxcox(blz_df['review_count'])
print('정규분포에 가깝도록 하는 λ : ', bc_params)
print('변환된 값 : ', rc_bc)
```

    정규분포에 가깝도록 하는 λ :  -0.3528981448601063
    변환된 값 :  [2.03359708 1.09633902 1.22790118 ... 2.26975224 0.91069594 0.91069594]
    

<br>

다음 포스팅에서는 원본 데이터, 로그 변환된 데이터, 그리고 Box_Cox변환된 데이터를 비교하여 살펴볼 것이다.

<br>

---

### References

- Alice Zheng, Amanda Casari, 『Feature Engineering for Machine Learning - PRRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS』, O'Relly Media (2018)
- [UCI](https://archive.ics.uci.edu/ml/datasets/online+news+popularity)
- [wikipedia : variance-stabilizing-transformation](https://en.wikipedia.org/wiki/Variance-stabilizing_transformation)

