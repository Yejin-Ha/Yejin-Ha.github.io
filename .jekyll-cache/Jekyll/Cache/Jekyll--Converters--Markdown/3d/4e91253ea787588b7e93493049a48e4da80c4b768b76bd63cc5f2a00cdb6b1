I"('<h4 id="ml-with-python-2-지도-학습-알고리즘-6-1-신경망-모델다층-퍼셉트론">[ML with Python] 2. 지도 학습 알고리즘 (6-1) 신경망 모델(다층 퍼셉트론)</h4>
<ul>
  <li>본 포스팅은 지도 학습 알고리즘인 신경망 모델에 관한 기본적인 내용에 관하여 다룹니다.</li>
  <li>다층 퍼셉트론(<code class="language-plaintext highlighter-rouge">multilayer perceptrons, MLP</code>)</li>
</ul>

<hr />

<p>필요 라이브러리 import</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">graphviz</span>
</code></pre></div></div>

<hr />

<h4 id="신경망-모델_다층-퍼셉트론multilayer-perceptrons-mlp"><u>신경망 모델_다층 퍼셉트론(multilayer perceptrons, MLP)</u></h4>

<p>가장 기본적인 형태의 인공신경망(Artificial Neural Networks) 구조로는 대표적으로 <code class="language-plaintext highlighter-rouge">다층 퍼셉트론(MLP)</code>이 있다. <code class="language-plaintext highlighter-rouge">MLP</code>는 입력층과 출력층 사이에 하나 이상의 은닉층이 존재하는 신경망이다. 또한, <code class="language-plaintext highlighter-rouge">MLP</code>는 선형 모델의 일반화된 모습이라고도 볼 수 있다.</p>

<p><br /></p>

<p>기존 선형 모델의 예측 공식은 다음과 같았다. 여기서 <code class="language-plaintext highlighter-rouge">w</code>는 가중치, <code class="language-plaintext highlighter-rouge">x</code>는  입력 특성, 그리고 <code class="language-plaintext highlighter-rouge">ŷ </code>는 학습된 계수의 가중치 합이다.</p>
<blockquote>
  <p>ŷ = w[0] * x[0] + w[1] * x[1] + … + w[p] * x[p] + b</p>
</blockquote>

<p>이 선형 모델은 다음과 같이 노드와 연결선으로 표현할 수 있다.</p>
<ul>
  <li>노드(node) : 입력 특성 <code class="language-plaintext highlighter-rouge">x</code>, 예측 <code class="language-plaintext highlighter-rouge">ŷ</code></li>
  <li>연결선 : 가중치 <code class="language-plaintext highlighter-rouge">w</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_logistic_regression_graph</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99989155-a3023380-2df5-11eb-863d-d0ec81b751ea.JPG" alt="그림5" /></p>

<p>선형 모델에서 더 나아가 <code class="language-plaintext highlighter-rouge">MLP</code>에서는 가중치 합을 만드는 과정을 여러 번 반복되며, <code class="language-plaintext highlighter-rouge">은닉 유닛(hidden unit)</code>이 함께 구성되어 있다. 그리고 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_single_hidden_layer_graph</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99989143-a1387000-2df5-11eb-9ab6-207d24d50c7c.JPG" alt="그림6" /></p>

<p>위의 노드와 연결선 신경망은 아래와 같이 가중치를 표시하였을 때</p>

<p><img width="400" alt="2" src="https://user-images.githubusercontent.com/53929665/99986439-83b5d700-2df2-11eb-87f5-604fddf0ab9c.jpg" /></p>

<p><br /></p>

<p>여기서 <code class="language-plaintext highlighter-rouge">입력층</code>이 <code class="language-plaintext highlighter-rouge">가중치</code>가 곱해져서 <code class="language-plaintext highlighter-rouge">은닉층</code>에 도달하게되면 다음과 같은 계산이 수행된다.</p>

<p><img src="https://user-images.githubusercontent.com/53929665/99986710-de4f3300-2df2-11eb-9c79-c5368da3e299.jpg" alt="그림3" /></p>

<p><br /></p>

<p>마지막으로, <code class="language-plaintext highlighter-rouge">은닉층</code>에서 다시 <code class="language-plaintext highlighter-rouge">가중치</code>를 거쳐 <code class="language-plaintext highlighter-rouge">출력층</code>에 도달하게되면 다음과 같이 계산되는 것을 확인할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/53929665/99986814-076fc380-2df3-11eb-8d40-b015412f3645.jpg" alt="그림4" /></p>

<p><br /></p>

<p>하지만, 여러 개의 가중치 합을 계산하는 것은 수학적으로 보면 하나의 가중치 합을 계산하는 것과 동일하기 때문에 <code class="language-plaintext highlighter-rouge">선형 함수</code>로 <code class="language-plaintext highlighter-rouge">은닉층</code>을 여러번 추가하더라도 <code class="language-plaintext highlighter-rouge">은닉층</code>을 1회 추가했을 때와 별반 차이가 없을 것이다. 따라서, <code class="language-plaintext highlighter-rouge">은닉층</code>과 <code class="language-plaintext highlighter-rouge">출력층</code>의 뉴런에서 출력값을 결정하는 함수인 <code class="language-plaintext highlighter-rouge">활성화 함수</code>는 선형 함수가 아닌 <code class="language-plaintext highlighter-rouge">비선형 함수</code>(ex. <code class="language-plaintext highlighter-rouge">시그모이드 함수</code> / <code class="language-plaintext highlighter-rouge">렐루</code> / <code class="language-plaintext highlighter-rouge">하이퍼볼릭탄젠트</code> 등)여야 한다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/53929665/99986429-831d4080-2df2-11eb-8c7f-f8cbdb104172.jpg" alt="그림1" /></p>

<p><br /></p>

<p><b><참고>&lt;/b&gt;</참고></b></p>

<p><code class="language-plaintext highlighter-rouge">렐루</code> 함수의 경우 아래에서 확인할 수 있듯이 0 이하의 값을 모두 잘라버리고, <br /><code class="language-plaintext highlighter-rouge">tanh</code> 함수는 낮은 입력값에 대해서는 -1로 수렴하고, 큰 입력값에 대해서는 +1로 수렴한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">line</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"tanh"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"best"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"x"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"relu(x), tanh(x)"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'relu(x), tanh(x)')
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99989225-b4e3d680-2df5-11eb-917c-adc7e7f28f68.png" alt="ML_Neural-Network_10_1" /></p>

<p><br /></p>

<p>따라서, 우리가 개별적으로 정해야 할 것은 <code class="language-plaintext highlighter-rouge">은닉층의 유닛 개수</code>이다. <code class="language-plaintext highlighter-rouge">은닉층</code>은 아래의 노드와 같이 계속해서 추가되어 질 수 있다. 소규모 데이터 셋일 경우 10개 정도로도 충분하지만 매우 복잡한 데이터셋에서는 10000개가 될 수도 있다. 이와 같이 <code class="language-plaintext highlighter-rouge">많은 은닉충</code>으로 구성된 대규모 신경망이 생기면 이를 <code class="language-plaintext highlighter-rouge">딥러닝</code>이라고 부른다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_two_hidden_layer_graph</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99989151-a2699d00-2df5-11eb-9511-7e43c26f53f5.JPG" alt="그림7" /></p>

<p><br /></p>

<hr />

<h3 id="references">References</h3>

<ul>
  <li>안드레아스 뮐러, 세라 가이도, 『파이썬 라이브러리를 활용한 머신러닝』, 박해선, 한빛미디어(2017)</li>
</ul>

:ET