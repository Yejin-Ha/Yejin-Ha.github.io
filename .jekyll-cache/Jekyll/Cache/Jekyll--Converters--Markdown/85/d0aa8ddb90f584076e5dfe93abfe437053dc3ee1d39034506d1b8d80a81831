I"C<h4 id="ml-with-python-2-지도-학습-알고리즘-2-선형-모델---릿지회귀">[ML with Python] 2. 지도 학습 알고리즘 (2) 선형 모델 - 릿지회귀</h4>
<ul>
  <li>본 포스팅은 지도 학습 알고리즘인 선형 모델에 관한 기본적인 내용에 관하여 다룹니다.</li>
  <li>릿지 회귀( <code class="language-plaintext highlighter-rouge">Ridge</code> )</li>
</ul>

<hr />

<p>필요 라이브러리 import</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</code></pre></div></div>

<hr />

<h4 id="리지-회귀"><u>리지 회귀</u></h4>

<p><code class="language-plaintext highlighter-rouge">리지(Ridge)</code>도 회귀를 위한 <code class="language-plaintext highlighter-rouge">선형 모델</code>이므로 <code class="language-plaintext highlighter-rouge">최소적합법</code>에서 사용한 것과 같은 예측 함수를 사용한다.<br />
하지만, <code class="language-plaintext highlighter-rouge">리지회귀</code>에서의 <b>가중치(<code class="language-plaintext highlighter-rouge">w</code>)</b>선택은 <u>훈련 데이터를 잘 예측</u>하기 위해서 뿐만 아니라 <u>추가 제약 조건을 만족</u>시키기 위한 목적도 있다.</p>

<p>여기서 추가 제약 조건이란</p>

<blockquote>
  <p>가중치의 절댓값을 가능한 작게 만드는 것</p>
</blockquote>

<p>이다.</p>

<p>다시 말해서 <code class="language-plaintext highlighter-rouge">w</code>의 모든 원소가 0에 가깝게 되길 원하는 것이다.<br />
즉, 직관적으로 생각하면 <u>모든 특성이 출력에 주는 영향을 최소한</u>으로 만드는 것이다.</p>

<p>이런 제약을 <code class="language-plaintext highlighter-rouge">규제(regularization)</code>이라고 한다.<br />
<code class="language-plaintext highlighter-rouge">규제</code>란 <code class="language-plaintext highlighter-rouge">과대적합</code>이 되지않도록 모델을 강제로 제한한다는 의미이다.</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">리지회귀</code>에서 사용하는 규제방식은 <code class="language-plaintext highlighter-rouge">L2규제</code>이다.</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">리지 회귀</code>는 <code class="language-plaintext highlighter-rouge">linear_model.Ridge</code>에 구현되어 있다.<br />
리지 회귀가 보스턴 주택가격 데이터셋에 어떻게 적용되는지 살펴보자</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">ridge01</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 점수: {:.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ridge01</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 점수: {:.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ridge01</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 점수: 0.67
테스트 세트 점수: 0.66
</code></pre></div></div>

<p>결과는</p>

<ul>
  <li>In <code class="language-plaintext highlighter-rouge">훈련 세트</code> : LinearRegression &gt; Ridge</li>
  <li>In <code class="language-plaintext highlighter-rouge">테스트 세트</code> : LinearRegression &lt; Ridge</li>
</ul>

<p>따라서, <code class="language-plaintext highlighter-rouge">선형회귀</code>에서는 데이터셋이 <code class="language-plaintext highlighter-rouge">과대적합</code>되었지만, <code class="language-plaintext highlighter-rouge">Ridge</code>는 덜 자유로운 모델이기 때문에 <code class="language-plaintext highlighter-rouge">과대적합</code>이 적어진다. <u>모델의 복잡도가 낮아지면 훈련 세트에서의 성능이 나빠지지만 더 `일반화된 모델`</u>이 된다는 강점이 있다.</p>

<p>따라서, <code class="language-plaintext highlighter-rouge">Ridge</code>는 덜 자유로운 모델이기 때문에 과대적합이 적어집니다. 모델의 복잡도가 낮아지면 훈련 세트에서의 성능은 나빠지지만 더 일반화된 모델이 됩니다. 
관심 있는 것은 테스트 세트에 대한 성능이기 때문에<code class="language-plaintext highlighter-rouge">LinearRegression</code>보다 <code class="language-plaintext highlighter-rouge">Ridge</code> 모델을 선택해야 합니다.</p>

<p>더불어, 사용자는 <code class="language-plaintext highlighter-rouge">Ridge</code>모델의 <code class="language-plaintext highlighter-rouge">alpha</code> 매개변수로 훈련세트의 성능 대비 모델을 얼마나 단순화할지를 지정할 수 있다. 앞에서는 alpha = 0.1을 사용했지만 해당 값이 최적이라고는 할 수 없다. 최적의 alpha 값은 사용하는 데이터셋에 달려있다.</p>

<p><u>`alpha`값을 높이면 계수를 0에 더 가깝게 만들어서 훈련 세트의 성능은 나빠지지만 일반화에는 도움을 줄 수 있다.</u></p>

<p>또한 <code class="language-plaintext highlighter-rouge">alpha</code>의 값에 따라 모델의 <code class="language-plaintext highlighter-rouge">coef_</code>속성(즉 <code class="language-plaintext highlighter-rouge">w</code>계수)이 어떻게 달라지는지를 알아보면 다음과같다.<br />
(전체가 최소가된다는 개념을 가지고 생각하면 이해하기 쉽다)</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">alpha</code>값이 높아짐 -&gt; <code class="language-plaintext highlighter-rouge">coef_</code>의 절댓값 크기가 작아짐</li>
  <li><code class="language-plaintext highlighter-rouge">alpha</code>값이 작아짐 -&gt; <code class="language-plaintext highlighter-rouge">coef_</code>의 절대값 크가가 커짐</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">load_extended_boston</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ridge10</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">ridge</span>   <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">ridge01</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge10</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'^'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Ridge alpha=10"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'s'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Ridge alpha=1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge01</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'v'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Ridge alpha=0.1"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"LinearRegression"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"params category"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"params size"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x2556ee71a00&gt;
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/98567615-7c230800-22f3-11eb-9b54-bb7e568d02cf.png" alt="linear_model_Ridge_15_1" /></p>

<p>이 그림에서 x축은 <code class="language-plaintext highlighter-rouge">coef_</code>의 원소를 위치대로 나열한 것이다.<br />
즉 <code class="language-plaintext highlighter-rouge">x = 0</code>는 첫 번째 특성에 연관된 계수이고, <code class="language-plaintext highlighter-rouge">x = 1</code>은 두 번째 특성에 연관된 계수이다.<br />
이런 식으로 <code class="language-plaintext highlighter-rouge">x = 100</code>까지 계속된다. y축은 각 계수의 수치를 나타낸다.</p>

<ul>
  <li>alpha = 10 일때, 대부분의 계수는 -3과 3사이에 위치한다.</li>
  <li>alpha = 1 일때, Ridge 모델의 계수는 alpha = 10일때 보다 커진다.</li>
  <li>alpha = 0.1 일때, 계수는 더 커진다.</li>
  <li>아무런 규제가 없는(alpha = 0) 즉 선형 회귀의 계수는 값이 더 커져 그림 밖으로 넘어간다.</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">규제</code>의 효과를 이해하는 또 다른 방법은 <u>`alpha`값을 고정하고 훈련 데이터의 크기를 변화시켜 보는 것</u>이다.</p>

<p>다음 그림은 보스턴 주택가격 데이터셋에서 여러 가지 크기로 샘플링하여<br />
<code class="language-plaintext highlighter-rouge">LinearRegression</code>과 <code class="language-plaintext highlighter-rouge">Ridge(alpha = 1)</code>을 적용한 것이다.<br />
(데이터셋의 크기에 따른 모델의 성능 변화를 나타낸 그래프를 <code class="language-plaintext highlighter-rouge">학습곡선(learning curve)</code>라 한다.)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_ridge_n_samples</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/98567620-7d543500-22f3-11eb-987e-749e2526b6e7.png" alt="linear_model_Ridge_20_0" /></p>

<p>예상대로 모든 데이터셋에 대해<br />
<code class="language-plaintext highlighter-rouge">릿지</code>와 <code class="language-plaintext highlighter-rouge">선형회귀</code> 모두 훈련 세트의 점수가 테스트 세트의 점수보다 높다.</p>

<p><code class="language-plaintext highlighter-rouge">릿지</code>에는 규제가 적용되므로 릿지의 훈련 데이터 점수가 전체적으로 <code class="language-plaintext highlighter-rouge">선형 회귀</code>의 훈련 데이터 점수보다 낮다.</p>

<p>그러나, 테스트 데이터에서는 <code class="language-plaintext highlighter-rouge">릿지</code>의 점수가 더 높으며 특별히 작은 데이터셋에서는 더 그렇다.</p>

<p>데이터셋 크기가 400미만에서는 <code class="language-plaintext highlighter-rouge">선형 회귀</code>는 어떤 것도 학습하지 못하고 있다.<br />
(이는 400미만의 값에서는 선형회귀는 과적합을 일으키기에 릿지회귀가 더 적합한 모델임을 의미한다)<br />
두 모델의 성능은 데이터가 많아질수록 좋아지고 마지막에는 <code class="language-plaintext highlighter-rouge">선형 회귀</code>가 <code class="language-plaintext highlighter-rouge">릿지 회귀</code>를 따라잡는다.<br />
여기서 중요한 점은 데이터를 충분히 주면 규제 항은 덜 중요해져서 <code class="language-plaintext highlighter-rouge">릿지 회귀</code> 와 <code class="language-plaintext highlighter-rouge">선형 회귀</code>의 성능이 같아질 것이라는 점이다.</p>

<p>마지막으로, <code class="language-plaintext highlighter-rouge">선형 회귀</code>의 훈련 데이터 성능이 데이터가 많아질수록 감소하는 것 또한 관측된다.<br />
이는 데이터가 많아질수록 모델이 데이터를 기억하거나 과대적합하기 어려워지기 때문이다.</p>

<hr />

<h3 id="references">References</h3>

<ul>
  <li>안드레아스 뮐러, 세라 가이도, 『파이썬 라이브러리를 활용한 머신러닝』, 박해선, 한빛미디어(2017)</li>
  <li><a href="https://tensorflow.blog/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%eb%a8%b8%ec%8b%a0%eb%9f%ac%eb%8b%9d/2-3-3-%ec%84%a0%ed%98%95-%eb%aa%a8%eb%8d%b8/">https://tensorflow.blog/파이썬-머신러닝/2-3-3-선형-모델/</a></li>
</ul>

:ET