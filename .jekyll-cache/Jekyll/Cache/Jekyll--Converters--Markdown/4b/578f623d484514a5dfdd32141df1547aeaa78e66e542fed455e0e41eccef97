I":N<h4 id="ml-with-python-2-지도-학습-알고리즘-4-앙상블---그래디언트-부스팅">[ML with Python] 2. 지도 학습 알고리즘 (4) 앙상블 - 그래디언트 부스팅</h4>
<ul>
  <li>본 포스팅은 지도 학습 알고리즘인 앙상블-그래디언트 부스팅에 관한 기본적인 내용에 관하여 다룹니다.</li>
  <li>앙상블 (<code class="language-plaintext highlighter-rouge">ensemble</code>)</li>
  <li>그래디언트 부스팅 (<code class="language-plaintext highlighter-rouge">Gradient Boosting</code>)</li>
  <li>그래디언트 부스팅의 장단점과 매개변수</li>
</ul>

<hr />

<p>필요 라이브러리 import</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
</code></pre></div></div>

<hr />

<h4 id="앙상블ensemble"><u>앙상블(ensemble)</u></h4>

<ul>
  <li>여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법이다.</li>
  <li><code class="language-plaintext highlighter-rouge">분류</code>와 <code class="language-plaintext highlighter-rouge">회귀 문제</code>의 다양한 데이터셋에서 효과적인 <code class="language-plaintext highlighter-rouge">앙상블 모델</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">랜덤 포레스트(random forest)</code></li>
      <li><code class="language-plaintext highlighter-rouge">그래디언트 부스팅(gradient boosting)</code></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<hr />

<h4 id="그래디언트-부스팅-회귀-트리gradient-boosting-regression-tree"><u>그래디언트 부스팅 회귀 트리(Gradient Boosting Regression Tree)</u></h4>

<ul>
  <li>여러 개의 결정 트리를 묶어 강력한 모델을 만드는 또 다른 <code class="language-plaintext highlighter-rouge">앙상블</code> 방법이다.</li>
  <li><u>이름은 회귀지만 `회귀`와 `분류`에 모두 사용할 수 있다.</u></li>
  <li><code class="language-plaintext highlighter-rouge">랜덤 포레스트</code>와 달리<br /> <code class="language-plaintext highlighter-rouge">그래디언트 부스팅</code>은 이전 <u>트리의 오차를 보완하는 방식</u>을 이용해 <u>순차적</u>으로 <code class="language-plaintext highlighter-rouge">트리</code>를 만든다.<br />또한, <u>무작위성을 가지고 있지 않다. 대신 `사전 가지치기`가 사용</u>된다.</li>
  <li>또한, <code class="language-plaintext highlighter-rouge">랜덤 포레스트</code>보다는 매개변수 설정에 조금 더 민감하지만,<br />잘 조정하면 더 높은 정확도를 보여준다.</li>
</ul>

<p><br /></p>

<p><code class="language-plaintext highlighter-rouge">그래디언트 부스팅</code>의 기본 아이디어는<br />
<code class="language-plaintext highlighter-rouge">얕은 트리</code>와 같은 간단한 모델(<code class="language-plaintext highlighter-rouge">약한 학습기(weak learner)</code>)를 많이 연결하는 것이다.<br />
따라서, 각각의 <code class="language-plaintext highlighter-rouge">트리</code>는 데이터의 일부에 대해서만 예측을 잘 수행 할 수 있어서<br />
<code class="language-plaintext highlighter-rouge">트리</code>가 많이 추가될수록 성능이 좋아진다.</p>

<p><br /></p>

<p><code class="language-plaintext highlighter-rouge">그래디언트 부스팅</code>에서 중요한 매개변수는<br />
<u>이전 트리의 오차를 얼마나 강하게 보정할 것인지를 제어</u>하는 <code class="language-plaintext highlighter-rouge">learning_rate</code>이다.<br />
<code class="language-plaintext highlighter-rouge">learning_rate</code>가 크면 트리의 오차 보정을 강하게 하기 때문에 복잡한 모델을 생성한다.<br />
또한, <code class="language-plaintext highlighter-rouge">n_estimators</code>값을 키우면 <code class="language-plaintext highlighter-rouge">앙상블</code>에 트리가 더 많이 추가 되어 모델의 복잡도가 커지고 훈련 세트에서의 실수를 바로 잡을 기회가 더 많아진다.</p>

<p><br /></p>

<p>아래는 <code class="language-plaintext highlighter-rouge">유방암 데이터셋</code>을 이용해 <code class="language-plaintext highlighter-rouge">GradientBoostingClassifier</code>를 사용한 예이다.<br />
기본값인 깊이가 3인 트리 100개와 <code class="language-plaintext highlighter-rouge">learning_rate</code> 0.1을 사용했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 1.000
테스트 세트 정확도: 0.965
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">훈련 세트</code>의 정확도가 100%이므로 <code class="language-plaintext highlighter-rouge">과대적합</code>으로 추정된다.<br />
<code class="language-plaintext highlighter-rouge">과대적합</code>을 막기 위해서</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">최대 깊이(max_depth)</code>를 줄이거나</li>
  <li><code class="language-plaintext highlighter-rouge">사전 가지치기</code>를 강하게 하거나</li>
  <li><code class="language-plaintext highlighter-rouge">학습률(learning_rate)</code>를 낮출 수 있다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 최대깊이를 줄이는 경우
</span><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 0.991
테스트 세트 정확도: 0.972
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 학습률을 낮추는 경우
</span><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 0.988
테스트 세트 정확도: 0.965
</code></pre></div></div>

<p>위의 두 방식은 복잡도를 감소시키기에, <code class="language-plaintext highlighter-rouge">훈련 세트</code>의 정확도가 낮아졌다.<br /></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">학습률을 낮추는 것</code>은 테스트 세트의 성능을 조금밖에 개선 못했지만,<br /></li>
  <li><code class="language-plaintext highlighter-rouge">트리의 최대 깊이를 낮추는 것</code>은 모델 성능 향상에 크게 기여했다.</li>
</ul>

<p><br /></p>

<p>다른 결정 트리 기반의 모델처럼 특성의 중요도를 시각화하면 모델을 더 잘 이해할 수 있다.<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">barh</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">model</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"feature_importances"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"property"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>

<span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">gbrt</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99261549-379dec00-2860-11eb-92cc-63799563699b.png" alt="ML_decision_tree_ensemble_gradientboosting_9_0" /></p>

<p><a href="https://jhryu1208.github.io/data/2020/11/16/ML_decision_tree_ensemble_random_forest/">이전 포스팅</a>의 <code class="language-plaintext highlighter-rouge">랜덤 포레스트</code>와 <code class="language-plaintext highlighter-rouge">그레디언트 부스팅</code>의 특성 중요도가 비슷한 것으로 확인된다. 다만 <code class="language-plaintext highlighter-rouge">그래디언트 부스팅</code>은 일부 특성을 완전히 무시한다.</p>

<p><br /></p>

<p>비슷한 종류의 데이터에서 <code class="language-plaintext highlighter-rouge">그래디언트 부스팅</code>과 <code class="language-plaintext highlighter-rouge">랜덤 포레스트</code> 둘다 잘 작동된다.<br />
보통은 더 안정적인 <code class="language-plaintext highlighter-rouge">랜덤 포레스트</code>를 먼저 적용한다.<br />
아무리 <code class="language-plaintext highlighter-rouge">랜덤 포레스트</code>가 잘 작동하더라도 예측 시간이 중요하거나<br />
머신러닝 모델에서 마지막 성능까지 쥐어 짜야 할 때는 <code class="language-plaintext highlighter-rouge">그레디언 부스팅</code>이 도움이 된다.</p>

<p><br /></p>

<hr />

<h4 id="장단점과-매개변수"><u>장단점과 매개변수</u></h4>

<p><code class="language-plaintext highlighter-rouge">그래디언트 부스팅</code> 결정 트리는 지도학습에서 가장 강력하고 널리 사용하는 모델 중 하나이다.<br /></p>

<ul>
  <li><b>장점</b>
    <ul>
      <li>다른 트리 기반 모델처럼 특성의 스케일을 조정하지 않아도 된다.</li>
      <li>이진(binary) 특성이나 연속적인 특성에서도 잘 동작한다.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><b>단점</b>
    <ul>
      <li>매개변수를 잘 조정해야만 한다.</li>
      <li>훈련 시간이 길다.</li>
      <li>트리 기반 모델의 특성상 희소한 고차원 데이터에는 잘 작동하지 못한다.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><b>그래디언트 부스팅 트리 모델의 매개변수</b>
    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">n_estimators</code> : 트리의 개수 지정</p>

        <ul>
          <li>해당 매개변수가 클수록 <code class="language-plaintext highlighter-rouge">랜덤 포레스트</code>와 달리<br /> <code class="language-plaintext highlighter-rouge">그래디언 부스팅</code>에서는 <code class="language-plaintext highlighter-rouge">모델이 복잡</code>해지고 <code class="language-plaintext highlighter-rouge">과대적합</code>될 가능성이 높아진다.</li>
        </ul>
      </li>
    </ul>

    <p><br /></p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">learning_rate</code> : 이전 트리의 오차를 보정하는 정도</p>

        <ul>
          <li><code class="language-plaintext highlighter-rouge">n_estimators</code>와 이 매개변수는 매우 깊게 연관되어있으며 해당 변수를 낮추면 비슷한 복잡도의 모델을 만들기 위해서는 <code class="language-plaintext highlighter-rouge">n_estimators</code>를 늘려서 더 많은 트리를 추가해야한다.</li>
          <li>일반적인 관례는 가용한 시간과 메모리 한도에서 <code class="language-plaintext highlighter-rouge">n_estimators</code>를 맞추고 나서 적절한 <code class="language-plaintext highlighter-rouge">learning_rate</code>를 찾는 것이다.</li>
        </ul>
      </li>
    </ul>

    <p><br /></p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">max_depth(or max_leaf_nodes)</code> : 트리의 복잡도를 지정</p>

        <ul>
          <li>통상 <code class="language-plaintext highlighter-rouge">그래디언트 부스팅</code>모델에서는 이 매개변수를 매우 작게 설정하며<br />트리의 깊이가 5보다 깊어지지 않게 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<hr />

<h3 id="references">References</h3>

<ul>
  <li>안드레아스 뮐러, 세라 가이도, 『파이썬 라이브러리를 활용한 머신러닝』, 박해선, 한빛미디어(2017)</li>
  <li><a href="https://tensorflow.blog/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%eb%a8%b8%ec%8b%a0%eb%9f%ac%eb%8b%9d/2-3-6-%ea%b2%b0%ec%a0%95-%ed%8a%b8%eb%a6%ac%ec%9d%98-%ec%95%99%ec%83%81%eb%b8%94/">https://tensorflow.blog/파이썬-머신러닝/2-3-6-결정-트리의-앙상블/</a></li>
</ul>

:ET