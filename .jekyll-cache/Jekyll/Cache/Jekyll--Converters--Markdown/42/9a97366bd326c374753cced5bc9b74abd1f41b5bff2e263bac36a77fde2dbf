I"U<h4 id="ml-with-python-3-비지도-학습-알고리즘-3-2-병합-군집">[ML with Python] 3. 비지도 학습 알고리즘 (3-2) 병합 군집</h4>
<ul>
  <li>본 포스팅은 k-평균 군집에 관한 기본적인 내용에 관하여 다룹니다.</li>
  <li>병합 군집(<code class="language-plaintext highlighter-rouge">agglomerative clustering</code>)</li>
  <li>계층형 군집과 덴드로그램</li>
</ul>

<hr />

<p>필요 라이브러리 import</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rc</span><span class="p">(</span><span class="s">'font'</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s">'Malgun Gothic'</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<p><br /></p>

<h4 id="병합-군집-agglomerative-clustering"><u>병합 군집 (agglomerative clustering)</u></h4>

<ul>
  <li>기본 원리 순서
    <ul>
      <li><b>STEP 1</b> : 각 포인트를 하나의 클러스터로 지정</li>
      <li><b>STEP 2</b> : 어떤 종료 조건을 만족할 때까지 가장 비슷한 두 클러스터를 계속 합침</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">linkage</code> 옵션 : 가장 비슷한 클러스터를 측정하는 방법 지정
    <ul>
      <li><code class="language-plaintext highlighter-rouge">ward</code> : <u>분산을 가장 적게 증가시키는</u> 두 클러스터를 합친다.</li>
      <li><code class="language-plaintext highlighter-rouge">average</code> : <u>포인트 사이의 평균 거리가 가장 짧은</u> 두 클러스터를 합친다.</li>
      <li><code class="language-plaintext highlighter-rouge">complete</code> : <u>클러스터 포인트 사이의 최대 거리가 가장 짧은</u> 두 클러스터를 합친다.</li>
      <li>일반적인 경우에는 <code class="language-plaintext highlighter-rouge">ward</code>가 대부분의 데이터셋에 알맞지만, 클러스터에 속한 포인트 수가 많이 다를 때는 (ex.클러스터의 크기가 매우 다를 때) <code class="language-plaintext highlighter-rouge">average</code>나 <code class="language-plaintext highlighter-rouge">complete</code>가 나을 수 있다. 즉 linkage 옵션은 데이터 타입 별로 적합성이 다를 수 있다.</li>
      <li>이와 관련해서는 해당 <a href="https://bizzengine.tistory.com/152">블로그</a>에서 시각적으로 잘 정리되어있다.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p>예시를 통해 이를 이해하면 다음과 같다. 다음은 <code class="language-plaintext highlighter-rouge">ward</code>옵션을 적용한 <code class="language-plaintext highlighter-rouge">병합 군집</code>의 경과를 보여준다. 초기에는 각 하나의 포인트가 클러스터를 이루고 있지만, 각 단계가 거듭할 때 마다, 클러스터가 합쳐지는 것을 확인할 수 있다. 그리고 마지막 단계에서 세 개의 클러스터를 찾는다고 지정했을 때 클러스터 3개만을 구성하며 알고리즘이 멈추게 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_agglomerative_algorithm</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/102802347-d5ee1600-43f9-11eb-88b2-f9f89741760d.png" alt="ML_agglomerative_clustering_6_0" /></p>

<p><br /></p>

<p>알고리즘 특성상 <code class="language-plaintext highlighter-rouge">병합 군집</code>은 새로운 데이터 포인트에 대해서는 예측할 수 없다. 따라서 <code class="language-plaintext highlighter-rouge">predict</code> 메소드가 해당 알고리즘에는 존재하지 않는다. 대신 훈련 세트로 모델을 만들고 클러스터 소속 정보를 얻기 위해서 <code class="language-plaintext highlighter-rouge">fit_predict</code> 메소드를 사용한다. 이전 <code class="language-plaintext highlighter-rouge">K평균 군집</code>에서 봤듯이, <code class="language-plaintext highlighter-rouge">fit_predict</code>는 훈련 세트를 학습시킨 뒤 얻은 레이블들을 반환한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">agg</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">assignment</span> <span class="o">=</span> <span class="n">agg</span><span class="p">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">mglearn</span><span class="p">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">assignment</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">"클러스터 0"</span><span class="p">,</span> <span class="s">"클러스터 1"</span><span class="p">,</span> <span class="s">"클러스터 2"</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">"best"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"특성 0"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"특성 1"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, '특성 1')
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/102802351-d71f4300-43f9-11eb-8723-fe35d1870839.png" alt="ML_agglomerative_clustering_8_2" /></p>

<p>위와 같이 결과를 완벽하게 도출하였다. <code class="language-plaintext highlighter-rouge">병합 군집</code>모델을 사용하려면 찾을 클러스터의 개수를 지정해주는 것을 위의 코드에서 확인할 수 있었다. <u>이렇게 사람들이 자의적으로 지정해줘야만 할 것 같은 클러스터의 개수는 의외로 `병합 군집`에서는 적절한 개수를 선택하는데 도움을 주기도 한다.</u></p>

<p><br /></p>

<hr />

<h4 id="계층형-군집과-덴드로그램"><u>계층형 군집과 덴드로그램</u></h4>

<p><code class="language-plaintext highlighter-rouge">병합 군집</code>은 <code class="language-plaintext highlighter-rouge">게층형 군집(hierarchical clustering)</code>을 만든다. 이것은 계층적 트리 모형을 이용해 개별 객체들을 순차적/계층적으로 유사한 개체 내지 그룹과 통합하여 군집화를 수행하는 알고리즘이다. 이는 가능한 모든 클러스터를 연결해보는 데 큰 도움이 된다. 또한, <code class="language-plaintext highlighter-rouge">SciPy</code>의 <code class="language-plaintext highlighter-rouge">덴드로그램 (dendrogram)</code>을 통하여 직관적으로 나타낼 수 있다. 더불어, <code class="language-plaintext highlighter-rouge">SciPy</code>는 데이터 배열 X를 받아 계층 군집의 유사도가 들어있는 연결 배열 반환 함수를 제공하기도 한다(ex. <code class="language-plaintext highlighter-rouge">ward()</code>, <code class="language-plaintext highlighter-rouge">average()</code>, <code class="language-plaintext highlighter-rouge">complete()</code>). 이 연결 배열을 <code class="language-plaintext highlighter-rouge">dendrogram</code> 함수에 넣어 덴드로그램 그래프를 시각화 할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SciPy에서 ward 군집 함수와 덴드로그램 함수를 임포트합니다
</span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">ward</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># 데이터 배열 X 에 ward 함수를 적용합니다
# SciPy의 ward 함수는 병합 군집을 수행할 때 생성된
# 거리 정보가 담긴 배열을 리턴합니다
</span><span class="n">linkage_array</span> <span class="o">=</span> <span class="n">ward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># 클러스터 간의 거리 정보가 담긴 linkage_array를 사용해 덴드로그램을 그립니다
</span><span class="n">dendrogram</span><span class="p">(</span><span class="n">linkage_array</span><span class="p">)</span>

<span class="c1"># 두 개와 세 개의 클러스터를 구분하는 커트라인을 표시합니다
</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">get_xbound</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bounds</span><span class="p">,</span> <span class="p">[</span><span class="mf">7.25</span><span class="p">,</span> <span class="mf">7.25</span><span class="p">],</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bounds</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">7.25</span><span class="p">,</span> <span class="s">' 두 개 클러스터'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">})</span>
<span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">4</span><span class="p">,</span> <span class="s">' 세 개 클러스터'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">})</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"샘플 번호"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"클러스터 거리"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, '클러스터 거리')
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/102802354-d71f4300-43f9-11eb-969f-c1c4170dfcd9.png" alt="ML_agglomerative_clustering_12_1" /></p>

<p>덴드로그램에서 살펴봐야 할 것은 다음과 같다.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">y축의 높이</code> : 노드가 만들어진 순서 파악 가능</li>
  <li><code class="language-plaintext highlighter-rouge">가지의 길이</code> : 클러스터가 얼마나 멀리 떨어져 있는지 파악 가능
    <ul>
      <li>가지가 가장 길다는 것은 꽤 먼 거리의 포인트를 모은다는 의미이다.</li>
    </ul>
  </li>
</ul>

<p>위의 덴드로그램은 최종적으로 1개의 군집으로 모든 데이타를 군집화 시키는 것을 확인할 수 있다. 따라서, 적정선의 특정 n개의 클러스터까지 군집으로 나눌 필요가 있다. 이떄, <code class="language-plaintext highlighter-rouge">sklearn</code>의 <code class="language-plaintext highlighter-rouge">fcluster</code>함수를 이용하면, 특정 클러스터 거리에서(<u>클러스터 갯수가 아니다!!!</u>)에서 클러스터링을 멈출 수 있다. 다음의 코드를 확인해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sub</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">fcluster</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fcluster</span><span class="p">(</span><span class="n">linkage_array</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="s">'distance'</span><span class="p">))</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predict</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">predict</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'predict'</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'X0'</span> <span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="n">tolist</span><span class="p">(),</span>
                   <span class="s">'X1'</span> <span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="n">tolist</span><span class="p">(),</span>
                   <span class="s">'real_label'</span><span class="p">:</span> <span class="n">y</span><span class="p">.</span><span class="n">tolist</span><span class="p">()})</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="n">left_index</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="s">'left'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'condition'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'real_label'</span><span class="p">]</span><span class="o">==</span><span class="n">df</span><span class="p">[</span><span class="s">'predict'</span><span class="p">])</span>
<span class="n">df</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X0</th>
      <th>X1</th>
      <th>real_label</th>
      <th>predict</th>
      <th>condition</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.549347</td>
      <td>0.692505</td>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.926358</td>
      <td>4.152430</td>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.005875</td>
      <td>4.387241</td>
      <td>2</td>
      <td>0</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.120314</td>
      <td>5.758061</td>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.737308</td>
      <td>4.425462</td>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2.368335</td>
      <td>0.043568</td>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.497722</td>
      <td>1.551282</td>
      <td>1</td>
      <td>2</td>
      <td>False</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-1.481145</td>
      <td>2.730698</td>
      <td>2</td>
      <td>2</td>
      <td>True</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.873051</td>
      <td>4.714386</td>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.662468</td>
      <td>2.175717</td>
      <td>2</td>
      <td>2</td>
      <td>True</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.742851</td>
      <td>1.463517</td>
      <td>2</td>
      <td>2</td>
      <td>True</td>
    </tr>
    <tr>
      <th>11</th>
      <td>2.499131</td>
      <td>1.231338</td>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>

<p>각 데이터 포인트의 예측값을 위에서 확인했을 때, 총 3개의 클러스터가 예측되는 것을 확인할 수 있다. 또한, 원본 데이터 레이블과 예측 레이블을 비교해보면  11개 중에서 2개를 제외하고 전부 휼륭히 예측하였다.</p>

<p><br /></p>

<p>하지만, 이렇게 유용해 보이는 <code class="language-plaintext highlighter-rouge">병합군집</code>은 two_moons와 같은 복잡한 형상은 구분하지 못한다. 그리고 데이터가 많아질수록 다른 클러스터 기법들에 비해 많이 느린편에 해당한다.</p>

<p><br /></p>

<hr />

<h3 id="references">References</h3>

<ul>
  <li>안드레아스 뮐러, 세라 가이도, 『파이썬 라이브러리를 활용한 머신러닝』, 박해선, 한빛미디어(2017)</li>
  <li><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html">fcluster 문서</a></li>
  <li><a href="https://bcho.tistory.com/1204">조대협의 블로그</a></li>
  <li><a href="https://ratsgo.github.io/machine%20learning/2017/04/18/HC/">ratio’s blog</a></li>
  <li><a href="https://bizzengine.tistory.com/152">인문계 공돌이</a></li>
</ul>

:ET