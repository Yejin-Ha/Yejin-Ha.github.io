I"\S<h4 id="ml-with-python-2-지도-학습-알고리즘-2-다중-클래스-분류">[ML with Python] 2. 지도 학습 알고리즘 (2) 다중 클래스 분류</h4>
<ul>
  <li>본 포스팅은 지도 학습 알고리즘인 분류용 선형 모델에 관한 기본적인 내용에 관하여 다룹니다.</li>
  <li>다중 클래스 분류용 선형 모델</li>
  <li>일대다 방법(<code class="language-plaintext highlighter-rouge">one-vs.-rest</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">SVC</code>를 이용한 일대다 방법</li>
  <li>선형 모델의 매개변수 (정리)</li>
  <li>선형 모델의 장단점</li>
</ul>

<hr />

<p>필요 라이브러리 import</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
</code></pre></div></div>

<hr />

<h4 id="다중-클래스-분류용-선형-모델"><u>다중 클래스 분류용 선형 모델</u></h4>

<p>(<code class="language-plaintext highlighter-rouge">로지스틱 회귀</code>만을 제외하고) 많은 <code class="language-plaintext highlighter-rouge">선형 분류 모델</code>은 태생적으로 <code class="language-plaintext highlighter-rouge">이진 분류</code>만을 지원한다.<br />
즉 <code class="language-plaintext highlighter-rouge">다중 클래스(multiclass)</code>를 지원하지 않는다는 의미이다. 그러므로, <code class="language-plaintext highlighter-rouge">이진 분류</code>알고리즘을 <code class="language-plaintext highlighter-rouge">다중 클래스 분류</code>알고리즘으로 확장하기 위해서 주로 <code class="language-plaintext highlighter-rouge">일대다 방법</code>을 사용한다.</p>

<p><br /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">일대다 방법(one-vs.-rest)</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">이진 분류</code>알고리즘을 <code class="language-plaintext highlighter-rouge">다중 클래스 분류</code> 알고리즘으로 확장하는 방법이다.</li>
      <li>각 클래스를 다른 모든 클래스와 구분하도록 <code class="language-plaintext highlighter-rouge">이진 분류</code>모델을 학습시킨다.</li>
      <li>결국 클래스의 수만큼 <code class="language-plaintext highlighter-rouge">이진 분류</code>모델이 만들어진다.</li>
      <li>예측할 때 이렇게 만들어진 모든 이진 분류기가 작동하여<br /> <u>가장 점수가 높은 점수를 내는 분류기의 클래스를 에측값으로 선택</u>한다.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p>클래스별 이진 분류기를 만들면 각 클래스가 계수 백터(<code class="language-plaintext highlighter-rouge">w</code>)와 절편(<code class="language-plaintext highlighter-rouge">b</code>)를 하나씩 가지게 된다.<br />
결국 <code class="language-plaintext highlighter-rouge">분류 신뢰도</code>를 나타내는 다음 공식의 <u>결과값이 가장 높은 클래스</u>가 해당 데이터의 클래스 레이블로 할당된다.</p>

<blockquote>
  <p>w[0] × x[0] + w[1] × x[1] + … + w[p] × x[p] + b</p>
</blockquote>

<p><br /></p>

<p>다음 세 개의 클래스를 가진 데이터셋에 <code class="language-plaintext highlighter-rouge">일대다 방식</code>을 적용해보자<br />
해당 데이터셋은 2차원이며 각 클래스의 데이터는 <code class="language-plaintext highlighter-rouge">정규분포(가우시안 분포)</code>를 따른다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">mglearn</span><span class="p">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"property 0"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"property 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">"class 0"</span><span class="p">,</span> <span class="s">"class 1"</span><span class="p">,</span> <span class="s">"class 2"</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x188ccf92880&gt;
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99145247-72f9b880-26b0-11eb-93b7-198cd3b2612c.png" alt="linearmodel_multi_classification_model_5_1" /></p>

<p><br /></p>

<p>먼저 <code class="language-plaintext highlighter-rouge">LinearSVC</code> 훈련결과를 확인해보자</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">linear_svm</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"계수 배열의 크기: "</span><span class="p">,</span> <span class="n">linear_svm</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"절편 배열의 크기: "</span><span class="p">,</span> <span class="n">linear_svm</span><span class="p">.</span><span class="n">intercept_</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[-0.17492627  0.23141057]
 [ 0.4762129  -0.0693673 ]
 [-0.18914077 -0.20400584]]
계수 배열의 크기:  (3, 2)

[-1.07745602  0.13140337 -0.08604988]
절편 배열의 크기:  (3,)
</code></pre></div></div>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">coef_</code>배열의 크기는 (3,2)이다.<br />
해당 배열의 행은 세 개의 클래스에 각각 대응하는 계수 벡터를 담고 있으며,<br />
열은 각 특성에 따른 계수 값을 가지고 있다.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">intercept_</code>는 각 클래스의 절편을 담고 있다.</p>
  </li>
</ul>

<p><br /></p>

<p>세 개의 이진 분류기가 만드는 경계를 시각화하면 다음과 같다.</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">-(line * coef[0] + intercept) / coef[1]</code><br />
<br />
위의 식은 기본적인 직선 공식(<code class="language-plaintext highlighter-rouge">Ax+By+C = 0</code>)에 기반되어있다.<br />
<br />
coef[0]x + coef[1]y + intercept = 0<br />
(coef[0]/coef[1])<em>x + y + intercept/coef[1] = 0<br />
y = -( (coef[0]/coef[1])</em>x + intercept/coef[1] )</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mglearn</span><span class="p">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">linear_svm</span><span class="p">.</span><span class="n">intercept_</span><span class="p">,</span>
                                  <span class="n">mglearn</span><span class="p">.</span><span class="n">cm3</span><span class="p">.</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"property 0"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"property 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'class 0'</span><span class="p">,</span> <span class="s">'class 1'</span><span class="p">,</span> <span class="s">'class 2'</span><span class="p">,</span> <span class="s">'class 0 boundary'</span><span class="p">,</span> <span class="s">'class 1 boundary'</span><span class="p">,</span>
            <span class="s">'class 2 boundary'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x188d3395910&gt;
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99145248-742ae580-26b0-11eb-95ed-3d3a16384cf0.png" alt="linearmodel_multi_classification_model_11_1" /></p>

<p>각각의 선이 이진 분류 모델을 뜻한다. 클래스의 수 만큼 이진 분류 모델이 만들어진 것을 확인할 수 있다. 이를 토대로 <code class="language-plaintext highlighter-rouge">decision boundary</code>가 그려진다.</p>

<p><br /></p>

<p>그렇다면 위의 그림 중앙의 세 분류기가 모두 나머지로 분류한 삼각형 영역은 어떻게되는 걸까? 해당 데이터 포인트는 가장 가까운 직선의 클래스가 될 것이다.</p>

<p><br /></p>

<p>다음 예는 2차원 평면의 모든 포인트에 대한 예측 결과를 보여준다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_2d_classification</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span>

<span class="n">mglearn</span><span class="p">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>


<span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">linear_svm</span><span class="p">.</span><span class="n">intercept_</span><span class="p">,</span>    
                                   <span class="n">mglearn</span><span class="p">.</span><span class="n">cm3</span><span class="p">.</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

    
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'class 0'</span><span class="p">,</span> <span class="s">'class 1'</span><span class="p">,</span> <span class="s">'class 2'</span><span class="p">,</span> <span class="s">'class 0 boundary'</span><span class="p">,</span> <span class="s">'class 1 boundary'</span><span class="p">,</span>
            <span class="s">'class 2 boundary'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"property 0"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"property 1"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'property 1')
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/53929665/99145249-742ae580-26b0-11eb-96da-11c0a6cb3425.png" alt="linearmodel_multi_classification_model_13_1" /></p>

<hr />

<h4 id="선형-모델의-매개변수"><u>선형 모델의 매개변수</u></h4>

<p>선형 모델의 주요 매개변수는 다음과 같았다.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">회귀 모델</code> : alpha
    <ul>
      <li>alpha ↑ =&gt; 모델이 단순해짐</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">LinearSVC</code>와 <code class="language-plaintext highlighter-rouge">LogisticRegression</code> : C
    <ul>
      <li>C ↓ =&gt; 모델이 단순해짐</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p>특별히 <code class="language-plaintext highlighter-rouge">회귀 모델</code>에서는 이 매개변수를 조정하는 것이 매우 중요하다.<br />
보통 <code class="language-plaintext highlighter-rouge">C</code>와 <code class="language-plaintext highlighter-rouge">alpha</code>는 <code class="language-plaintext highlighter-rouge">로그 스케일</code>로 최적치를 정한다.</p>

<p><br /></p>

<p>그리고 <code class="language-plaintext highlighter-rouge">L1 규제</code>를 사용할지 <code class="language-plaintext highlighter-rouge">L2 규제</code>를 사용할지도 정해야한다.<br /></p>

<ul>
  <li>중요한 특성이 많지 않음 =&gt; <code class="language-plaintext highlighter-rouge">L1 규제</code>
    <ul>
      <li>특히, 해당 규제는 모델의 해석이 중요한 요소일 떄도 사용할 수 있다.<br />몇 가지 특성만 사용하므로 해당 모델에 중요한 특성이 무엇이고 그 효과가 어느 정도인지 설명하기 쉬워진다.</li>
    </ul>

    <p><br /></p>
  </li>
  <li>중요한 특성이 많음 =&gt; <code class="language-plaintext highlighter-rouge">L2 규제</code></li>
</ul>

<hr />

<h4 id="선형-모델의-장단점"><u>선형 모델의 장단점</u></h4>

<p><b>장점</b></p>

<ul>
  <li>학습 속도와 예측이 빠르다.</li>
</ul>

<p><br /></p>

<ul>
  <li>매우 큰 데이터셋과 희소한 데이터셋에서도 잘 작동한다.
    <ul>
      <li>수십만에서 수백만 개의 샘플로 이뤄진 대용량 데이터셋이라면 기본 설정보다 빨리 처리하도록 <code class="language-plaintext highlighter-rouge">LogisticRegression</code>과 <code class="language-plaintext highlighter-rouge">Ridge</code>에 <code class="language-plaintext highlighter-rouge">solver='sag'</code> 옵션을 준다.</li>
      <li>다른 대안으로 <code class="language-plaintext highlighter-rouge">SDGClassifier</code>와 <code class="language-plaintext highlighter-rouge">SGDRegressor</code>을 사용할 수 있다.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>회귀와 분류에서 본 공식을 사용해 예측이 어떻게 만들어지는지 비교적 쉽게 이해할 수 있다.</li>
</ul>

<p><br /></p>

<ul>
  <li>샘플에 비해 특성이 많을 때 잘 작동한다.<br />다른 모델로 학습하기 어려운 매우 큰 데이터셋에도 선형 모델을 많이 사용한다.</li>
</ul>

<p><b>단점</b></p>

<ul>
  <li>위에서 처럼 예측이 어떻게 만들어지는지 비교적 쉽게 이해할 수 있지만,<br />계수의 값들이 왜 그런지는 명확하지 않을 때가 종종있다.<br />특히 데이터셋의 특성들이 서로 깊게 연관되어 있을 때 그렇다.<br />그리고 이런 경우 계수 분석이 매우 어려울 수 있다.</li>
</ul>

<p><br /></p>

<ul>
  <li>특성이 적은 저차원 데이터셋에서는 다른 모델들의 일반화 성능이 더 좋다.</li>
</ul>

<hr />

<h3 id="references">References</h3>

<ul>
  <li>안드레아스 뮐러, 세라 가이도, 『파이썬 라이브러리를 활용한 머신러닝』, 박해선, 한빛미디어(2017)</li>
  <li><a href="https://tensorflow.blog/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%eb%a8%b8%ec%8b%a0%eb%9f%ac%eb%8b%9d/2-3-3-%ec%84%a0%ed%98%95-%eb%aa%a8%eb%8d%b8/">https://tensorflow.blog/파이썬-머신러닝/2-3-3-선형-모델/</a></li>
  <li><a href="https://kolikim.tistory.com/9">https://kolikim.tistory.com/9</a></li>
</ul>

:ET